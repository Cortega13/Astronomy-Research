{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "WORKING_PATH = \"C:/Users/carlo/Projects/Astronomy Research/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA = [\"Time\", \"Model\"]\n",
    "PHYSICAL_PARAMETERS = np.loadtxt(os.path.join(WORKING_PATH, \"Main Scripts 11.9.24/utils/physical_parameters.txt\"), dtype=str, delimiter=\" \").tolist()\n",
    "TOTAL_SPECIES = np.loadtxt(os.path.join(WORKING_PATH, \"Main Scripts 11.9.24/utils/species.txt\"), dtype=str, delimiter=\" \").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4,5])\n",
    "\n",
    "print(x/x)\n",
    "print(10**x)\n",
    "print(np.log10(10**x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(os.path.join(WORKING_PATH, \"Datasets/training.h5\"), \"autoencoder\", start=0, stop=1000)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index().loc[20].sort_values(ascending=False)[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'A': range(1, 11), 'B': range(11, 21)}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['pair_group'] = df.index // 2\n",
    "\n",
    "grouped = df.groupby('pair_group')\n",
    "\n",
    "pair_means = grouped.mean()\n",
    "\n",
    "for name, group in grouped:\n",
    "    print(f\"Pair {name}:\\n\", group, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.read_hdf(os.path.join(WORKING_PATH, \"Datasets/training.h5\"), \"emulator\", start=0, stop=5)\n",
    "\n",
    "df = np.log10(df)\n",
    "\n",
    "global_min = df.min().min()  # minimum value across all columns\n",
    "global_max = df.max().max()  # maximum value across all columns\n",
    "\n",
    "#scaled_df = (df - global_min) / (global_max - global_min)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(\n",
    "    np.array([global_min, global_max]).reshape(-1, 1)\n",
    ")\n",
    "print(scaler.data_min_)\n",
    "\n",
    "\n",
    "\n",
    "scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [f\"Component_{num}\" for num in range(1, 12)]\n",
    "components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "INPUT: training.h5, validation.h5, abundances.scalers\n",
    "OUTPUT: autoencoder.pth\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from joblib import load\n",
    "import torch\n",
    "from torch import nn, optim, multiprocessing as mp\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "### Configurations\n",
    "WORKING_PATH = \"C:/Users/carlo/Projects/Astronomy Research/\"\n",
    "HP = {\n",
    "    \"encoded_dimensions\": 11,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"weight_decay\": 0,\n",
    "    \"batch_size\": 2*4096,\n",
    "    \"shuffle\": True,\n",
    "    \"gradient_clipping\": 100,\n",
    "    \"early_stopping_tolerance\": 7,\n",
    "    \"max_epochs\": 999999,\n",
    "    \"hidden_layer\": 5\n",
    "}\n",
    "\n",
    "METADATA = [\"Time\", \"Model\"] \n",
    "PHYSICAL_PARAMETERS = np.loadtxt(os.path.join(WORKING_PATH, \"Main Scripts 11.9.24/utils/physical_parameters.txt\"), dtype=str, delimiter=\" \").tolist()\n",
    "TOTAL_SPECIES = np.loadtxt(os.path.join(WORKING_PATH, \"Main Scripts 11.9.24/utils/species.txt\"), dtype=str, delimiter=\" \").tolist()\n",
    "\n",
    "\n",
    "### Data Processing Functions\n",
    "def load_datasets(path):\n",
    "    training_dataset_path = os.path.join(path, \"Datasets/training.h5\")\n",
    "    validation_dataset_path = os.path.join(path, \"Datasets/validation.h5\")\n",
    "\n",
    "    training_dataset = pd.read_hdf(training_dataset_path, \"autoencoder\", start=0, stop=1000).astype(np.float32)\n",
    "    validation_dataset = pd.read_hdf(validation_dataset_path, \"autoencoder\", start=0, stop=1000).astype(np.float32)\n",
    "\n",
    "    training_dataset = training_dataset[TOTAL_SPECIES]\n",
    "    validation_dataset = validation_dataset[TOTAL_SPECIES]\n",
    "\n",
    "    return training_dataset, validation_dataset\n",
    "\n",
    "\n",
    "def autoencoder_preprocessing(abundances_features):\n",
    "    ### Preprocesses the data for the autoencoder training. Returns a dataloader.\n",
    "    print(\"Starting Encoder Preprocessing.\")\n",
    "\n",
    "    # Created using only the training data.\n",
    "    scalers = load(os.path.join(WORKING_PATH, \"Datasets/scalers.plk\"))\n",
    "    abundances_min, abundances_max = scalers[\"total_species\"]\n",
    "\n",
    "    # Log10 Scale Abundances and then MinMax scale.\n",
    "    abundances_features = np.log10(abundances_features, dtype=np.float32)\n",
    "\n",
    "    # Minmax scale all the abundances_features.\n",
    "    abundances_features = (abundances_features - abundances_min) / (abundances_max - abundances_min)\n",
    "\n",
    "\n",
    "    return abundances_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_PATH = \"C:/Users/carlo/Projects/Astronomy Research/\"\n",
    "\n",
    "training_dataset, validation_dataset = load_datasets(WORKING_PATH)\n",
    "\n",
    "training = autoencoder_preprocessing(training_dataset)\n",
    "training_dataset\n",
    "row = training_dataset.loc[1]\n",
    "\n",
    "# Sort the values in decreasing order, placing NaN first\n",
    "sorted_row = row.sort_values(ascending=False, na_position='first')\n",
    "\n",
    "sorted_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from joblib import load\n",
    "WORKING_PATH = \"C:/Users/carlo/Projects/Astronomy Research/\"\n",
    "\n",
    "scalers = load(os.path.join(WORKING_PATH, \"Datasets/scalers.plk\"))\n",
    "\n",
    "scalers[\"encoded_components\"] = (\n",
    "    0,\n",
    "    1\n",
    ")\n",
    "\n",
    "print(scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_PATH = \"C:/Users/carlo/Projects/Astronomy Research/\"\n",
    "training_dataset_path = os.path.join(WORKING_PATH, \"Datasets/training.h5\")\n",
    "training_dataset = pd.read_hdf(training_dataset_path, \"autoencoder\", start=0).astype(np.float32)\n",
    "\n",
    "z = training_dataset.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rows, cols = 100000, 340\n",
    "data = np.random.rand(rows, cols).astype(np.float32)\n",
    "df = pd.DataFrame(data, dtype=np.float32)\n",
    "data1 = df.to_numpy()\n",
    "\n",
    "# Check if they are now equal\n",
    "print(np.array_equal(data, data1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log10, floor\n",
    "\n",
    "def round_sig(x, sig=3):\n",
    "    return round(x, sig-int(floor(log10(abs(x))))-1)\n",
    "\n",
    "\n",
    "# data = {\n",
    "#     'A': [0.02345253401234, 123453.456, 12.34523e-15, 0.987654, 0.988454],\n",
    "#     'B': [0.987654, 1.234252452e-4, 2e-15, 0.987654, 0.988454],\n",
    "# }\n",
    "# df = pd.DataFrame(data, dtype=np.float32)\n",
    "\n",
    "df_rounded1 = df.map(lambda x: round_sig(x, 3))\n",
    "df_rounded1 = df_rounded1.astype(np.float32)\n",
    "#mask = df_rounded.duplicated(keep=\"first\")\n",
    "#df[~mask]\n",
    "#df\n",
    "\n",
    "#2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log10, floor\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def round_sig(x, sig=3):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    return round(x, sig - int(floor(log10(abs(x)))) - 1)\n",
    "\n",
    "@njit\n",
    "def round_array(arr, sig=3):\n",
    "    result = np.empty_like(arr)\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            result[i, j] = round_sig(arr[i, j], sig)\n",
    "    return result\n",
    "\n",
    "# Apply rounding function\n",
    "rounded_data = round_array(data, sig=3)\n",
    "\n",
    "# Create a rounded DataFrame\n",
    "df_rounded2 = pd.DataFrame(rounded_data, dtype=np.float32)\n",
    "df_rounded2 = df_rounded2.astype(np.float32)\n",
    "print(df_rounded1.equals(df_rounded2))\n",
    "print(df_rounded1)\n",
    "print(df_rounded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.31e-05]]\n"
     ]
    }
   ],
   "source": [
    "x = 1.3146541*10**-5\n",
    "\n",
    "test = np.array([[x]])\n",
    "\n",
    "print(round_array(test, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Generate large DataFrame on GPU\n",
    "rows, cols = 100000, 340\n",
    "data_gpu = torch.rand((rows, cols), dtype=torch.float32, device='cuda')\n",
    "\n",
    "def round_sig_torch(x, sig=3):\n",
    "    \"\"\"Rounds a tensor to a specified number of significant figures.\"\"\"\n",
    "    log_abs = torch.log10(torch.abs(x))\n",
    "    scale = sig - torch.floor(log_abs) - 1\n",
    "    factor = torch.pow(10, scale)\n",
    "    rounded = torch.round(x * factor) / factor\n",
    "    return rounded\n",
    "\n",
    "# Apply rounding\n",
    "rounded_data_gpu = round_sig_torch(data_gpu, sig=3)\n",
    "\n",
    "# Convert back to pandas DataFrame on CPU (optional)\n",
    "rounded_data_cpu = rounded_data_gpu.cpu().numpy()\n",
    "df_rounded = pd.DataFrame(rounded_data_cpu, dtype=np.float32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
